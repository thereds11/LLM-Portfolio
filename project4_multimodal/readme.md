# Project 4: Multimodal Applications

This project demonstrates the multimodal capabilities of Large Language Models, specifically using OpenAI's GPT-4o to generate descriptive alternative text for images. It showcases how LLMs can understand and interpret visual content.

## Purpose

The primary purpose is to illustrate the power of multimodal LLMs in processing both text and image inputs. This can be applied to various use cases such as accessibility (generating alt text for visually impaired users), content generation, and image understanding.

## How to Run

1.  **Obtain an OpenAI API Key:**
    You need an OpenAI API key with access to GPT-4o. You can get one from the [OpenAI Platform](https://platform.openai.com/).
2.  **Set your API Key:**
    Replace `'sk-proj-...'` in `app.py` with your actual OpenAI API key, or preferably, set it as an environment variable named `OPENAI_API_KEY`.
    ```python
    # In app.py, replace this line:
    OPENAI_API_KEY = 'your_openai_api_key_here'
    # Or set as environment variable before running:
    # export OPENAI_API_KEY='your_openai_api_key_here' (Linux/macOS)
    # $env:OPENAI_API_KEY='your_openai_api_key_here' (PowerShell)
    ```
3.  **Navigate to the project directory:**
    ```bash
    cd D:/Sirius/projects/AI_Portfolio/project4_multimodal
    ```
4.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    # On Windows
    .\venv\Scripts\activate
    # On macOS/Linux
    source venv/bin/activate
    ```
5.  **Install dependencies:**
    ```bash
    pip install openai Pillow requests matplotlib llama-index-llms-openai
    ```
    *(Note: A `requirements.txt` is not provided in this specific project, so these are the core dependencies based on `app.py`)*
6.  **Run the application:**
    ```bash
    python app.py
    ```
    The script will fetch images from predefined URLs and print their descriptions generated by GPT-4o. It also attempts to display the images using `matplotlib` (commented out by default).

## Techniques Used

*   **OpenAI API (GPT-4o):** The core of the project, utilizing GPT-4o's multimodal capabilities to process images and generate text descriptions.
*   **LlamaIndex `ChatMessage` and `ImageBlock`:** Used to structure the input to the LLM, combining text prompts with image data.
*   **`requests` and `Pillow`:** For fetching images from URLs and handling image processing.
*   **Asynchronous Operations:** `asyncio` and `astream_chat` are used to handle asynchronous API calls, allowing for streaming responses.

## Issues and Considerations

*   **API Key Security:** Never hardcode your API key in production code. Use environment variables or a secure configuration management system.
*   **API Costs:** Using powerful models like GPT-4o incurs costs based on usage. Be mindful of your API consumption.
*   **Rate Limits:** OpenAI APIs have rate limits. For large-scale processing, consider implementing retry mechanisms and exponential backoff.
*   **Image URLs:** The project uses hardcoded image URLs. For a more robust application, you would need a mechanism to upload or specify image paths dynamically.
*   **Error Handling:** Basic error handling is present, but more comprehensive error management might be needed for production.

## Contributions

Feel free to contribute to this project by opening issues or submitting pull requests.
